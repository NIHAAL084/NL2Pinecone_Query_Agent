nihaalanupoju@Nihaals-MacBook-Air NL2Pinecone_Query_Agent % make docker-build
üê≥ Building Docker image...
docker build -t nl2pinecone-agent .
[+] Building 88.6s (20/20) FINISHED                   docker:desktop-linux
 => [internal] load build definition from Dockerfile                  0.1s
 => => transferring dockerfile: 2.23kB                                0.0s
 => [internal] load metadata for ghcr.io/astral-sh/uv:latest          1.3s
 => [internal] load metadata for docker.io/library/python:3.11-slim   2.0s
 => [auth] library/python:pull token for registry-1.docker.io         0.0s
 => [internal] load .dockerignore                                     0.0s
 => => transferring context: 2B                                       0.0s
 => FROM ghcr.io/astral-sh/uv:latest@sha256:2fd1b38e3398a256d6af3f71  0.1s
 => => resolve ghcr.io/astral-sh/uv:latest@sha256:2fd1b38e3398a256d6  0.1s
 => [builder 1/7] FROM docker.io/library/python:3.11-slim@sha256:139  0.1s
 => => resolve docker.io/library/python:3.11-slim@sha256:139020233cc  0.1s
 => [internal] load build context                                     0.4s
 => => transferring context: 602.51kB                                 0.4s
 => CACHED [builder 2/7] COPY --from=ghcr.io/astral-sh/uv:latest /uv  0.0s
 => CACHED [builder 3/7] WORKDIR /app                                 0.0s
 => CACHED [builder 4/7] RUN apt-get update && apt-get install -y     0.0s
 => [builder 5/7] COPY pyproject.toml uv.lock* README.md ./           0.2s
 => [builder 6/7] RUN uv venv /opt/venv --python 3.11                 0.7s
 => [builder 7/7] RUN uv sync --no-dev --frozen                       3.7s
 => CACHED [runtime 3/7] RUN apt-get update && apt-get install -y     0.0s
 => CACHED [runtime 4/7] COPY --from=builder /opt/venv /opt/venv      0.0s
 => CACHED [runtime 5/7] WORKDIR /app                                 0.0s
 => [runtime 6/7] COPY . .                                            6.0s
 => [runtime 7/7] RUN useradd -m -u 1000 appuser &&     chown -R ap  65.2s
 => exporting to image                                                9.5s
 => => exporting layers                                               5.8s
 => => exporting manifest sha256:f63ea082893ea3638fcbd619600556491a6  0.0s
 => => exporting config sha256:707f3b41fd27b3753df75df991c706e20b078  0.0s
 => => exporting attestation manifest sha256:920dd26a3f271ad4b16341e  0.1s
 => => exporting manifest list sha256:6c3587ef829f4243be9db6cac553d4  0.1s
 => => naming to docker.io/library/nl2pinecone-agent:latest           0.0s
 => => unpacking to docker.io/library/nl2pinecone-agent:latest        3.4s
‚úÖ Docker image built: nl2pinecone-agent
nihaalanupoju@Nihaals-MacBook-Air NL2Pinecone_Query_Agent % make docker-run

üê≥ Running Docker container...
üìç API will be available at: http://localhost:8000
docker run -d \
                --name nl2pinecone-api \
                -p 8000:8000 \
                --env-file .env \
                nl2pinecone-agent
fe331772213dc786152994b6cf86408b9386f25cf1c09e9ec8a2be8610b49eab
‚úÖ Container started. Check logs with: docker logs nl2pinecone-api
nihaalanupoju@Nihaals-MacBook-Air NL2Pinecone_Query_Agent % make test-all-e
ndpoints 
üöÄ Testing ALL API endpoints...
================================================

üìç 1. Testing GET / (Root endpoint)
curl -s http://localhost:8000/ | jq .
{
  "message": "Natural Language to Pinecone Query Agent API",
  "version": "1.0.0",
  "endpoints": {
    "/query": "POST - Convert natural language to Pinecone filter",
    "/results": "POST - Search Pinecone with natural language query",
    "/batch-query": "POST - Process multiple queries in batch",
    "/batch-results": "POST - Search Pinecone with multiple natural language queries",
    "/health": "GET - Health check",
    "/examples": "GET - Example queries and responses"
  }
}

üìç 2. Testing GET /health (Health check)
curl -s http://localhost:8000/health | jq .
{
  "status": "healthy",
  "service": "nl2pinecone-agent"
}

üìç 3. Testing GET /examples (Example queries)
curl -s http://localhost:8000/examples | jq .
{
  "examples": [
    {
      "query": "Show me articles by Alice Zhang from last year about machine learning.",
      "expected_filter": {
        "author": "Alice Zhang",
        "published_year": {
          "$eq": 2024
        },
        "tags": {
          "$in": [
            "machine learning"
          ]
        }
      }
    },
    {
      "query": "Find posts tagged with 'LLMs' published in June, 2023.",
      "expected_filter": {
        "tags": {
          "$in": [
            "LLMs"
          ]
        },
        "published_year": {
          "$eq": 2023
        },
        "published_month": {
          "$eq": 6
        }
      }
    },
    {
      "query": "Anything by John Doe on vector search?",
      "expected_filter": {
        "author": "John Doe",
        "tags": {
          "$in": [
            "vector search"
          ]
        }
      }
    }
  ]
}

üìç 4. Testing POST /query (Single query conversion)
curl -s -X POST "http://localhost:8000/query" \
                -H "Content-Type: application/json" \
                -d '{"query": "Show me articles by Alice Zhang from last year about machine learning"}' | jq .
{
  "original_query": "Show me articles by Alice Zhang from last year about machine learning",
  "pinecone_filter": {
    "author": "Alice Zhang",
    "tags": {
      "$in": [
        "machine learning"
      ]
    },
    "published_year": {
      "$eq": 2024
    }
  },
  "is_valid": true,
  "timestamp": "2025-07-10T07:01:37.563602"
}

üìç 5. Testing POST /batch-query (Batch query conversion)
curl -s -X POST "http://localhost:8000/batch-query" \
                -H "Content-Type: application/json" \
                -d '["Find posts tagged with LLMs published in June, 2023", "Anything by John Doe on vector search?"]' | jq .
{
  "results": [
    {
      "original_query": "Find posts tagged with LLMs published in June, 2023",
      "pinecone_filter": {
        "tags": {
          "$in": [
            "LLMs"
          ]
        },
        "published_year": {
          "$eq": 2023
        },
        "published_month": {
          "$eq": 6
        }
      },
      "is_valid": true,
      "timestamp": "2025-07-10T07:01:38.488975"
    },
    {
      "original_query": "Anything by John Doe on vector search?",
      "pinecone_filter": {
        "author": "John Doe",
        "tags": {
          "$in": [
            "vector search"
          ]
        }
      },
      "is_valid": true,
      "timestamp": "2025-07-10T07:01:39.534018"
    }
  ],
  "total_processed": 2
}

üìç 6. Testing POST /results (Vector search with filter - requires Pinecone)
Note: This endpoint requires Pinecone setup. Will return error if not configured.
curl -s -X POST "http://localhost:8000/results" \
                -H "Content-Type: application/json" \
                -d '{"query": "articles by John Doe about AI", "top_k": 3}' | jq . || echo "‚ùå Vector search unavailable (Pinecone not configured)"
{
  "original_query": "articles by John Doe about AI",
  "pinecone_filter": {
    "author": "John Doe",
    "tags": {
      "$in": [
        "AI"
      ]
    }
  },
  "results": [
    {
      "id": "sample-90",
      "score": 0.756990552,
      "metadata": {
        "author": "John Doe",
        "content": "John Doe's article provides an accessible overview of AI Transformers, highlighting their significance in recent advancements in natural language processing and computer vision. The piece explains the core concepts behind the transformer architecture, focusing on self-attention mechanisms and their ability to process information in parallel. Doe also discusses the practical applications of Transformers, showcasing their widespread use in areas like machine translation, text generation, and image recognition. Finally, the article touches upon the limitations and future directions of research surrounding these powerful AI models.",
        "published_day": 16.0,
        "published_month": 6.0,
        "published_year": 2024.0,
        "tags": [
          "AI",
          "transformers"
        ]
      }
    },
    {
      "id": "sample-81",
      "score": 0.725788414,
      "metadata": {
        "author": "John Doe",
        "content": "John Doe's article on transformers explores the architecture's groundbreaking impact on AI, particularly in natural language processing. It details how the self-attention mechanism allows models to weigh the importance of different words in a sequence, leading to significant improvements in understanding context. The piece highlights the versatility of transformers beyond language, noting their increasing application in computer vision and other domains. Ultimately, the article argues that transformers represent a pivotal advancement in AI, enabling more sophisticated and human-like machine learning capabilities.",
        "published_day": 10.0,
        "published_month": 5.0,
        "published_year": 2022.0,
        "tags": [
          "AI",
          "transformers"
        ]
      }
    },
    {
      "id": "sample-41",
      "score": 0.668342829,
      "metadata": {
        "author": "John Doe",
        "content": "John Doe's article highlights vector search as a crucial technology for unlocking the power of modern AI and deep learning models. By representing data as high-dimensional vectors, vector search enables efficient similarity comparisons, significantly improving the speed and accuracy of information retrieval. The piece emphasizes how this technique is particularly relevant for tasks involving unstructured data like images, text, and audio, allowing AI systems to understand and interact with real-world information more effectively. Doe suggests that continued advancements in vector search will be key to building more sophisticated and context-aware AI applications.",
        "published_day": 27.0,
        "published_month": 5.0,
        "published_year": 2025.0,
        "tags": [
          "vector search",
          "AI",
          "deep learning"
        ]
      }
    }
  ],
  "total_results": 3,
  "timestamp": "2025-07-10T07:01:42.600045"
}

üìç 7. Testing POST /batch-results (Batch vector search - requires Pinecone)
Note: This endpoint requires Pinecone setup. Will return error if not configured.
curl -s -X POST "http://localhost:8000/batch-results" \
                -H "Content-Type: application/json" \
                -d '{"queries": ["articles by John Doe about AI", "posts about machine learning"], "top_k": 2}' | jq . || echo "‚ùå Batch vector search unavailable (Pinecone not configured)"
{
  "results": [
    {
      "original_query": "articles by John Doe about AI",
      "pinecone_filter": {
        "author": "John Doe",
        "tags": {
          "$in": [
            "AI"
          ]
        }
      },
      "results": [
        {
          "id": "sample-90",
          "score": 0.756990552,
          "metadata": {
            "author": "John Doe",
            "content": "John Doe's article provides an accessible overview of AI Transformers, highlighting their significance in recent advancements in natural language processing and computer vision. The piece explains the core concepts behind the transformer architecture, focusing on self-attention mechanisms and their ability to process information in parallel. Doe also discusses the practical applications of Transformers, showcasing their widespread use in areas like machine translation, text generation, and image recognition. Finally, the article touches upon the limitations and future directions of research surrounding these powerful AI models.",
            "published_day": 16.0,
            "published_month": 6.0,
            "published_year": 2024.0,
            "tags": [
              "AI",
              "transformers"
            ]
          }
        },
        {
          "id": "sample-81",
          "score": 0.725788414,
          "metadata": {
            "author": "John Doe",
            "content": "John Doe's article on transformers explores the architecture's groundbreaking impact on AI, particularly in natural language processing. It details how the self-attention mechanism allows models to weigh the importance of different words in a sequence, leading to significant improvements in understanding context. The piece highlights the versatility of transformers beyond language, noting their increasing application in computer vision and other domains. Ultimately, the article argues that transformers represent a pivotal advancement in AI, enabling more sophisticated and human-like machine learning capabilities.",
            "published_day": 10.0,
            "published_month": 5.0,
            "published_year": 2022.0,
            "tags": [
              "AI",
              "transformers"
            ]
          }
        }
      ],
      "total_results": 2,
      "timestamp": "2025-07-10T07:01:43.788509"
    },
    {
      "original_query": "posts about machine learning",
      "pinecone_filter": {
        "tags": {
          "$in": [
            "machine learning"
          ]
        }
      },
      "results": [
        {
          "id": "sample-28",
          "score": 0.758706033,
          "metadata": {
            "author": "John Doe",
            "content": "John Doe's article clarifies the fundamental principles of machine learning, explaining how algorithms learn from data to make predictions or decisions without explicit programming. It highlights the difference between supervised, unsupervised, and reinforcement learning paradigms, providing practical examples of each. The article also addresses common challenges in machine learning, such as overfitting and bias, while offering potential solutions for improved model performance. Ultimately, Doe presents a balanced perspective on the potential and limitations of this rapidly evolving field.",
            "published_day": 18.0,
            "published_month": 12.0,
            "published_year": 2024.0,
            "tags": [
              "machine learning"
            ]
          }
        },
        {
          "id": "sample-98",
          "score": 0.734330475,
          "metadata": {
            "author": "Priya Patel",
            "content": "Sample article about machine learning, NLP by Priya Patel.",
            "published_day": 4.0,
            "published_month": 11.0,
            "published_year": 2023.0,
            "tags": [
              "machine learning",
              "NLP"
            ]
          }
        }
      ],
      "total_results": 2,
      "timestamp": "2025-07-10T07:01:44.838417"
    }
  ],
  "total_processed": 2,
  "timestamp": "2025-07-10T07:01:44.838481"
}

‚úÖ All endpoint tests completed!
================================================
nihaalanupoju@Nihaals-MacBook-Air NL2Pinecone_Query_Agent %